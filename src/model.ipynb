{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.w1 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.w2 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.w1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.w2(x)\n",
    "        x = x+identity\n",
    "        out = self.activation(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_ResNetBlock(nn.Module):\n",
    "    def __init__(self, n_features,r):\n",
    "        super(SE_ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.globalpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.fc = nn.Conv2d(in_channels=n_features,out_channels=n_features//r,kernel_size=1,stride=1,padding=0) #nn.Linear(in_features=n_features,out_features=n_features//r) \n",
    "        self.fc2 = nn.Conv2d(in_channels=n_features//r,out_channels=n_features,kernel_size=1,stride=1,padding=0) #nn.Linear(in_features=n_features//r,out_features=n_features)\n",
    "        self.gate = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        se = self.globalpool(out) #.unsqueeze(-1).unsqueeze(-1) add if using nn.linear\n",
    "        se = self.fc(se)\n",
    "        se = self.activation(se)\n",
    "        se = self.fc2(se)\n",
    "        se = self.gate(se)\n",
    "        \n",
    "        out = (out*se)+identity\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_ResNet(nn.Module):\n",
    "    def __init__(self, n_in, n_features, num_blocks=3,r=16):\n",
    "        super(SE_ResNet, self).__init__()\n",
    "        #First conv layers needs to output the desired number of features.\n",
    "        conv_layers = [nn.Conv2d(n_in, n_features, kernel_size=3, stride=1, padding=1),\n",
    "                       nn.ReLU()] #320x100\n",
    "        for i in range(num_blocks):\n",
    "            conv_layers.append(Se_ResNetBlock(n_features,r))\n",
    "            \n",
    "        conv_layers.append([nn.MaxPool2d(2,2),\n",
    "                            nn.Conv2d(n_features, 2*n_features, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU()]) #160x50\n",
    "        for i in range(num_blocks):\n",
    "            conv_layers.append(Se_ResNetBlock(2*n_features,r))\n",
    "            \n",
    "        conv_layers.append([nn.MaxPool2d(2,2),\n",
    "                            nn.Conv2d(2*n_features, 4*n_features, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU()]) #80x25\n",
    "        for i in range(num_blocks):\n",
    "            conv_layers.append(Se_ResNetBlock(4*n_features,r))\n",
    "            \n",
    "        self.blocks = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(80*25*4*n_features, 2048),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(2048, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,5),\n",
    "                                nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
