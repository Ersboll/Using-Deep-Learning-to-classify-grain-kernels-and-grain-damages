{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"The code will run on GPU. This is important so things run faster.\")\n",
    "else:\n",
    "    print(\"The code will run on CPU. You should probably not do this.\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset (Dataset):\n",
    "    def __init__(self,data,target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        temp = np.load(self.data[idx])\n",
    "        \n",
    "        X = torch.from_numpy(temp).type(torch.DoubleTensor)\n",
    "        y = torch.tensor(self.target[idx]).type(torch.LongTensor)\n",
    "        \n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load(\"data_preprocessed_path.npy\")\n",
    "labels=np.load(\"labels.npy\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "split = random_split(data,(50000,len(data)-50000))\n",
    "\n",
    "data_train = data[split[0].indices]\n",
    "data_test = data[split[1].indices]\n",
    "labels_train = labels[split[0].indices]\n",
    "labels_test = labels[split[1].indices]\n",
    "\n",
    "train_set = dataset(data_train,labels_train)\n",
    "test_set = dataset(data_test,labels_test)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True,num_workers=0)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.w1 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.w2 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        x = self.w1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.w2(x)\n",
    "        x = x+identity\n",
    "        out = self.activation(x)\n",
    "        return out\n",
    "    \n",
    "class SE_ResNetBlock(nn.Module):\n",
    "    def __init__(self, n_features,r):\n",
    "        super(SE_ResNetBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_features,out_channels=n_features,kernel_size=3,stride=1,padding=1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.globalpool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.fc = nn.Conv2d(in_channels=n_features,out_channels=n_features//r,kernel_size=1,stride=1,padding=0) #nn.Linear(in_features=n_features,out_features=n_features//r) \n",
    "        self.fc2 = nn.Conv2d(in_channels=n_features//r,out_channels=n_features,kernel_size=1,stride=1,padding=0) #nn.Linear(in_features=n_features//r,out_features=n_features)\n",
    "        self.gate = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        se = self.globalpool(out) #.unsqueeze(-1).unsqueeze(-1) add if using nn.linear\n",
    "        se = self.fc(se)\n",
    "        se = self.activation(se)\n",
    "        se = self.fc2(se)\n",
    "        se = self.gate(se)\n",
    "        \n",
    "        out = (out*se)+identity\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_ResNet(nn.Module):\n",
    "    def __init__(self, n_in, n_features, num_blocks=2,r=8):\n",
    "        super(SE_ResNet, self).__init__()\n",
    "        #First conv layers needs to output the desired number of features.\n",
    "        conv_layers =[nn.Conv2d(n_in, n_features, kernel_size=3, stride=1, padding=1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Conv2d(n_features,n_features,3,1,1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.MaxPool2d(2,2), #160x50\n",
    "                      nn.Conv2d(n_features,2*n_features,3,1,1),\n",
    "                      nn.ReLU()]\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            conv_layers.append(SE_ResNetBlock(2*n_features,r))\n",
    "            \n",
    "        conv_layers.append(nn.Sequential(nn.MaxPool2d(2,2),\n",
    "                            nn.Conv2d(2*n_features, 4*n_features, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU())) #80x25\n",
    "        \n",
    "        for i in range(num_blocks):\n",
    "            conv_layers.append(SE_ResNetBlock(4*n_features,r))\n",
    "            \n",
    "        conv_layers.append(nn.Sequential(nn.MaxPool2d(2,2),\n",
    "                            nn.Conv2d(4*n_features, 8*n_features, kernel_size=3, stride=1, padding=1),\n",
    "                            nn.ReLU())) #40x13 eller #40x12\n",
    "        for i in range(num_blocks):\n",
    "            conv_layers.append(SE_ResNetBlock(8*n_features,r))\n",
    "        \n",
    "        self.blocks = nn.Sequential(*conv_layers)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(40*12*8*n_features, 2048),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(2048, 512),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(512,5),\n",
    "                                nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        #reshape x so it becomes flat, except for the first dimension (which is the minibatch)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the training as a function so we can easily re-use it.\n",
    "def train(model, optimizer, num_epochs=10):\n",
    "    train_acc_all = []\n",
    "    test_acc_all = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), unit='epoch'):\n",
    "        model.train()\n",
    "        #For each epoch\n",
    "        train_correct = 0\n",
    "        for minibatch_no, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            #Zero the gradients computed for each weight\n",
    "            optimizer.zero_grad()\n",
    "            #Forward pass your image through the network\n",
    "            output = model(data)\n",
    "            #Compute the loss\n",
    "            loss = F.nll_loss(torch.log(output), target)\n",
    "            #Backward pass through the network\n",
    "            loss.backward()\n",
    "            #Update the weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Compute how many were correctly classified\n",
    "            predicted = output.argmax(1)\n",
    "            train_correct += (target==predicted).sum().cpu().item()\n",
    "            \n",
    "            #Remove mini-batch from memory\n",
    "            torch.cuda.empty_cache()\n",
    "            del data, target, loss\n",
    "        #Comput the test accuracy\n",
    "        test_correct = 0\n",
    "        model.eval()\n",
    "        for data, target in test_loader:\n",
    "            data = data.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(data)\n",
    "            predicted = output.argmax(1).cpu()\n",
    "            test_correct += (target==predicted).sum().item()\n",
    "        train_acc = train_correct/len(trainset)\n",
    "        test_acc = test_correct/len(testset)\n",
    "        train_acc_all.append(train_acc)\n",
    "        test_acc_all.append(test_acc)\n",
    "        print(\"Accuracy train: {train:.1f}%\\t test: {test:.1f}%\".format(test=100*test_acc, train=100*train_acc))\n",
    "    return test_acc_all, train_acc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SE_ResNet(n_in=7,n_features=8).double()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-3)\n",
    "train(model,optimizer,num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
